# Decoder Overview

In this file, you will get an idea of how the output sequence is generated by the decoder.

Let's say the output sequence is x y z.

The decoder initializes with a special token called [BOS] which indicates the beginning of the sentence.

### Step-by-Step Generation of the Output Sequence

1. Initialize the string with [BOS]:

2. Using the current state (last token generated by the decoder = [BOS]), the decoder predicts the next word x:

3. Now, the decoder generates a contextual embedding of the next token ('y') by taking the Decoder_Current_State ([BOS] x) and predicts the token from the embedding as y:

4. Now, the decoder generates a contextual embedding of the next token ('z') by taking the Decoder_Current_State ([BOS] x y) and predicts it as z:

5. Finally, the decoder generates a contextual embedding of the next token by taking the Decoder_Current_State ([BOS] x y z) and predicts it as [EOS] which represents the End of the sentence:


### Notes
- The decoder generates the output one token at a time and in a sequential manner; this nature is called auto-regressive nature.
- While generating the future token, the decoder uses the past token information under the pretext that it doesn't know about the future tokens.
