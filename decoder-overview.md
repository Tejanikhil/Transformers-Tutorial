Decoder Overview : 
In this file, you will get an idea of how the output sequence is generated by the decoder
Lets say the output sequence is x y z

The decoder initializes with a special token called [BOS] which indicates the beginning of the sentence
Lets consider the output string that is initialized is Decoder_Current_State = ""
step1 : Initialize the string with [BOS]
Decoder_Current_State = "[BOS]"
step2 : By using the current state (last token generated by the decoder = [BOS]) the decoder predicts the next word x
Decoder_Current_State = "[BOS] x"
step3 : Now, the decoder generates a contextual embedding of the next token ('y') by taking the decoder_current_state ([BOS] x) and predict the token from the embedding as y
Decoder_Current_State = "[BOS] x y"
step4 : Now, the decoder generates a contextual embedding of the next token ('z') by taking the decoder_current_state ([BOS] x y) and predict it as z
Decoder_Current_State = "[BOS] x y z"
step5 : Now, the decoder generates a contextual embedding of the next token by taking the decoder_current_state ([BOS] x y z) and predict it as [EOS] which represents the End of the sentence
Decoder_Current_State = "[BOS] x y z [EOS]"

Note : 
* The decoder generate the output one token at a time and in a sequential manner, this nature is called as auto-regressive nature.
* While generating the future token, the decoder uses the past token information in the pretext that it doesnt know about the future tokens. 
